## 标题

基于知识图谱的学术信息检索系统设计与实现

## 摘要

本设计主要做的工作的是研究以知识图谱技术为基础，实现一个学术信息检索系统，主要实学术信息定期爬取、学术信息更新、学术关联检索、知识化可视化界面等功能，分为服务器端和客户端两种用户。服务器端可以在网站后台进行管理，用户通过Web界面在客户端自由检索信息。由于学术信息在各个网站。因此,课题的研究对于提高老师教学水平和学生论文查阅效率，对于学校师生在查阅相关资料的具有一定的实际意义。该系统是在爬取知网数据基础上，引入了知识图谱的技术，采用自然语言处理，模式识别等技术。

## 关键词

文档检索，数据爬虫，自然语言处理，知识图谱

## 目录

> 写出目录，标明页码。

## 前言

随着互联网和大数据的快速发展，各式各样的信息资源呈现出“井喷式”增长趋势，对于这些急剧膨胀、组织无序的数字资源、网络资源，如何加强数字信息资源的开发和利用能力，如何应对纷繁复杂的搜索任务，如何更有效提升用户的信息搜索水平，成为国家和学术界关注的重大问题。检索学术信息主要途径有图书、期刊、论文、学术年会信息、会议论文、外文文献、学习论坛及网站信息等。

本课题是在导师的指导下，依据学校各方面硬件条件和自己的知识结构制定出来的。是建立在大量的文献资料和对文献资料的。以现在的信息技术发展为依据，是一个很有研究意义的课题。可用于学生查阅资料，老师教课等领域。

由于学术信息在各个网站。因此,课题的研究对于提高老师教学水平和学生论文查阅效率，对于学校师生在查阅相关资料的具有一定的实际意义。该系统是在爬取知网，万方等数据基础上，引入了知识图谱的技术，采用自然语言处理完成了对信息的检索。通过对其中的数据进行学术信息检索，师生关系查询，领域知识检索，科研项目查询，学术论坛，学术信息管理。

## 1 模块设计

这个学术信息检索系统采用的设计方案，从前往后依次用到了学术信息获取，对于获取到的学术信息进行数据处理，然后将经过处理后的学术信息存放到图数据库中，最后利用web做出一个类似于百度搜索引擎的检索系统。因此，本系统大致分为三个模块，首先第一个模块为数据爬取，第二个模块为数据处理，第三个模块是检索系统，第四个模块为web可视化。大体设计流程如下图所示：

![](https://cdn.jsdelivr.net/gh/eternidad33/picbed/img/知识图谱学术信息检索系统总体设计.png)

### 1.1 数据爬取

目的，利用Python爬虫技术对知网、万方大约爬取50000条可用数据。分析目标网站，明晰目标网站结构，理清关键数据位置。发起请求：使用http库或浏览器模拟工具向目标站点发起请求，即发送一个Request。获取响应内容。如果得到了一个Response，Response包含：html，json，图片，视频等。说明浏览器能正常响应。解析内容：解析html数据：正则表达式（RE模块），第三方解析库如Beautifulsoup，lxml等。解析json数据：json模块；通过解析html、json或其他数据，获得想要的关键数据信息，或者是下一个待爬取的URL地址。保存数据。虽然网络爬虫可以方便地为人们获取感性区的信息数据，但在进行网络爬虫时，亦应了解网络爬虫引发的问题。

针对不同页面Python语言有相应的方便简单的页面获取库，例如requests库及Selenium库，而对于比较大型的网站，则可以使用scrapy开源架构。但是本系统设计只需用到知网、万方的部分数据，所以就不考虑用scrapy了。知网是中国国内在目前最大的学术信息系统了，这个网站在1999年的时候就已经建立了，它是国家知识的一个超级巨大的知识资源的数据库，是清华大学的极为重要的项目，方便了无数学者去查阅资料，获取知识，并为广大学子提供了宝贵的论文实例。万方数据库也是一个学术信息的网站，但它的名气可能没有中国知网高，但是界面却是非常友好的，资源虽然没有中国知网的数据那么多，不过基本也能在上边查到自己想要找的文献资料，它是由万方数据开发的一个检索学术信息的网站。

根据本系统的设计要求，下面来介绍对于中国知网的相关学术信息数据的爬取，首先打开中国知网的首页，在网站页面的顶部的输入框输入关键字进行查询，比如输入“知识图谱”四个字，然后点击键盘上的回车进行查询，跳转到了新的页面，之后对其中页面的数据进行爬取，抓取页面中的文章列表信息，其中的文章列表信息包含文章标题、作者、来源、发表时间、数据库类型。这个页面只是通过搜索“知识图谱”关键字获取到文章列表的基本信息，然后点击文章的标题跳转到文章的详情页，可以获取到每篇文章的标题、摘要、关键词、资金资助、DOI、专辑、专题、分类号。点击到作者的详情页也可以获取到此作者的名字、所属机构、总发文量、总下载量、作者关注的领域，作者文献、作者导师、合作作者、获得的支持基金指导的学生，主讲视频等，暂时先将爬取到的数据存储到一个CSV文件中，之后对爬取到的数据进行一系列的处理。对于万方数据的爬取同样类似于知网的数据的爬取，只是爬取的标签发生了些许的改变，如下图为知网数据的爬取。

![知网数据的爬取](https://cdn.jsdelivr.net/gh/eternidad33/picbed/img/知网数据爬取.png)

### 1.2 数据处理

上一个模块已经获取到了知网和万方数据的相关学术信息，本模块的最终目的是将获取到的数据有条理的存储到Neo4j图数据库中的，本设计模块可以通过简单地分为三个阶段，处理CSV文件阶段，进行ORM映射阶段，将数据存储到Neo4j图数据库阶段。

处理CSV阶段，进行基本的数据处理步骤，比如传统的数据导入和清洗，采集好数据，肯定不少是重复或是无用的数据，此时需要通过数据对数据进行处理，将这些来自前端的数据导入到集中的大型分布式数据库，或者分布式存储集群，并进行简单的清洗和预处理工作。而这个过程当中最大的挑战就是导入的数据量大，经常会达到百兆，甚至千兆级别。数据统计和分析，统计与分析很多是需要用到工具来处理，比如可视化工具、spss工具、一些结构算法模型，分类汇总，满足企业的数据分析需求。这个过程最大的特点就是目的清晰，按照一定规则去分类汇总，才能得到有效分析，这部分处理起来也很占用系统资源。此阶段会用到的技术有Python中的numpy，pandas等模块，将处理后的数据导入有结构的CSV文件中。

进行ORM映射阶段，ORM是对象关系映射的简称，ORM模式是一种为了解决面向对象与关系数据库存在的互不匹配的现象的技术。简单的说，ORM是通过使用描述对象和数据库之间映射 的元数据，将程序中的对象自动持久化到关系数据库中。 ORM提高了开发效率，由于ORM可以自动对Entity对象与数据库中的Table进行字段与属性的映射，所以我们实际可能已经不需要一个专用的、庞大的数据访问层。 ORM提供了对数据库的映射，不用sql直接编码，能够像操作对象一样从数据库获取数据，如下图为简单的ER图。 

![](https://cdn.jsdelivr.net/gh/eternidad33/picbed/img/学术信息ER图.png)

将数据存储到Neo4j图数据库阶段，图形数据库是NoSQL数据库的一种类型，它应用图形理论存储实体之间的关系信息。图形数据库是一种非关系型数据库，它应用图形理论存储实体之间的关系信息。最常见例子就是社会网络中人与人之间的关系。关系型数据库用于存储“关系型”数据的效果并不好，其查询复杂、缓慢、超出预期，而图形数据库的独特设计恰恰弥补了这个缺陷。Neo4j是一个流行的图形数据库，它是开源的。最近，Neo4j的社区版已经由遵循AGPL许可协议转向了遵循GPL许可协议。尽管如此，Neo4j的企业版依然使用AGPL许可。Neo4j基于Java实现，兼容ACID特性，也支持其他编程语言，如Ruby和Python。本阶段的设计则是利用Python的py2neo将学术信息数据存储到Neo4j中。

常用的图数据库有Neo4j是一个高性能的,NOSQL图形数据库，它将结构化数据存储在网络上而不是表中。它是一个嵌入式的、基于磁盘的、具备完全的事务特性的Java持久化引擎，但是它将结构化数据存储在网络(从数学角度叫做图)上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。FlockDB是一个开源的分布式容错 图数据库，用于管理宽而浅的网络图。[3] Twitter最初用于存储用户之间的关系，例如关注和收藏。FlockDB与其他图形数据库不同，例如Neo4j，它不是为多跳图遍历而设计的，而是为快速设置操作而设计的，与Redis集的主要用例不同。[4]由于它仍处于Twitter使用之外的打包过程中，因此代码仍然非常粗糙，因AllegroGraph是Franz Lnz公司（Web语义产品提供商，旗舰产品是基于LISP的企业开发工具）的产品之一，Pfizer、Ford、Kodak、NASA和美国国防部都是该公司的客户。AllegroGraph是一个封闭源三元库，旨在存储RDF 三元组，这是一种标准格式的关联数据。AllegroGraph目前正在商业项目和国防部项目中使用。它也是TwitLogic项目的存储组件，它将语义Web引入Twitter数据。GraphDB是德国sones公司在.NET基础上构建的。Sones公司于2007年成立，近年来陆续进行了几轮融资。GraphDB社区版遵循AGPL v3许可协议，企业版是商业化的。GraphDB托管在Windows Azure平台上。InfiniteGraph基于Java实现，它的目标是构建“分布式的图形数据库”，已被美国国防部和美国中央情报局所采用。除此之外，还有其他一些图形数据库，如OrientDB、InfoGrid和HypergraphDB。Ravel构建在开源的Pregel实现之上，微软研究院的Trinity项目也是一个图形数据库项目。

### 1.3 信息检索

通过上一个模块的设计目的，已经将知网和万方数据获取到的学术信息进行了相关处理，并且存储到了Neo4j的图数据库中。本模块主要的目的为将用户输入的数据通过自然语言处理转换成对学术信息的提取，比如用户输入“基于网络爬虫的数据采集系统”，系统就会生成相应的网络爬虫和数据采集相关的文献资料，而且还可以查询师生关系，比如“张三的老师是谁”，系统就会返回张三的老师的相关信息，或者问“张三的学生是谁”，系统就会列出张三的学生，如果查询不到也会做出相应的提示，“对不起，暂时还查不到张三的学生信息”，如果提问“张三写过的文章有哪些”，系统则把作者为张三的文章列出来，虽然这些功能看似是用到的只是Neo4j的CQl语句进行查询，但其中也涉及到了一些自然语言处理的技术。关系型数据库的语句叫SQL，而图数据库Neo4j的语句叫做CQL，CQL代表Cypher查询语言，是一种声明性模式匹配语言，遵循SQL语法，语法是非常简单且人性化、可读的格式。 

信息检索模块类似于百度搜索引擎，信息检索系统作为网络信息平台的一个重要组成部分,在网上信息获取方面发挥了不可替代的作用,尤其是在机器学习、自然语言处理、知识表示和推理等人工智能技术被应用到信息采集、信息索引、查询处理、信息检索和排序、结果反馈等基本环节后，使得检索性能得到不断改善,信息检索系统已成为人们获取信息必不可少的工具，相关研究也取得了很大进展。但网上信息资源不断增多，内容和形式多样，信息之间的关联性也比较强,信息组织局部有序而整体无序，这些特点造成用户想要检索到所需要的准确信息仍然十分困难，因为当前的信息检索系统用户查询表达能力还十分有限，用户查询意图无法得到准确的理解，同时，在用户查询的处理上，要想使信息检索系统能够进行准确的分析，就必须使其具备自然语言理解能力，目前对自然语言的处理虽然已从语法阶段上升到语义阶段，但是仍然限制在一些规范的语句和语法范围内,不可避免地会造成语义上的丢失,而且大都没有考虑到对知识的整合，所以经常会输出大量无用的垃圾信息，智能性也不高。

自然语言处理( Natural Language Processing, NLP)是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分 。从长远来看，自然语言处理具有广阔的应用领域和前景，作为一门由计算机科学、人工智能和语言学三科融合的新兴领域，它的长远发展对每个学科都具有重大的意义和影响力。未来自然语言的发展趋势可能从人工构建知识到自动构建，人们可以利用一些显性知识构建一种方法，挖掘语言成分之间的关系，这样就避免了人工的繁琐和耗时。在文本的理解和推理层面可以由浅及深，完成对文本的深层次理解。哈尔滨工业大学刘挺教授在第三届中国人工智能大会上提到：可以使阅读理解作为一个深入探索自然语言理解的平台，Google 也已经推出了这样的测试机，也就是说让计算机理解一篇文章，接下来人类对计算机进行提问，观察计算机的回答能力完成测试。未来自然语言处理的发展趋势是 NLP 与许多领域的深度结合，从而为各相关行业创造价值。银行、电器和医学等领域对自然语言处理的需要都在日益提高，NLP+ 与各行业的结合越紧密，专业化的服务趋势就会越来越强。

### 1.4 Web可视化

通过前边几个模块的设计，已经基本完成了对于系统后台的设计，本模块则是利用后台设计出的功能接口，做出一个前台显示的页面，通俗的来说就是用户可以通过浏览器访问的页面。本模块需要用到Echarts，ECharts，一个纯 Javascript 的图表库，可以流畅的运行在 PC  和移动设备上，兼容当前绝大部分浏览器（IE8/9/10/11，Chrome，Firefox，Safari等），底层依赖轻量级的 Canvas  类库 ZRender，提供直观，生动，可交互，可高度个性化定制的数据可视化图表。

学术信息可视化是处理海量的学术信息大数据，本模块以ECharts技术为核心，结合实际中国知网和万方开发了一套Web可视化展示模块。主要是基于SVG链接文献，作者，机构，刊物之间的关系图，以及相关的标签之间的关系。该套可视化系统基于Web浏览器,兼容性好、维护成本低、易于开发，直观形象的展示了文献之间的关系状态,为学生和导师的工作提供了便利，本工程可应用于各个领域的学生和老师的教学工作，推动学术事业的发展。

可视化是将数据组织成易于为人所理解和认知的结构，然后用图形的方式形象地呈现出来的理论、方法和技术。实现可视化有两个关键要素，一个是数据，另一个是图形。从工作内容来看，前端工程师主要负责处理内容呈现和用户交互。可视化的数据呈现，尤其是在Web端的呈现，属于前端工程师的范畴。但是，与传统的前端开发相比，可视化开发的工具、思路和方法，又和Web有着比较大的区别。首先是技术栈的不同。Web开发主要以HTML来描述结构，以CSS来描述表现，以JavaScript来描述行为。而可视化则较少涉及HTML和CSS，它更多地要同浏览器的Canvas、SVG、WebGl等其他图形API打交道。其次，Web开发着重于处理普通的文本和多媒体信息，渲染普通的、易于阅读的文本和多媒体内容，而可视化开发则着重于处理结构化数据，渲染各种相对复杂的图表和图形元素。两者针对的信息特征和对图形渲染的要求有所不同，所以，在细节处理上也有比较大的差异。可视化要处理更多偏视觉方面的细节信息，尤其是在要呈现的数据细节比较丰富的时候，可能要精确地呈现大小、距离、角度、高度、光线、阴影等等。这些细节的处理，都需要你对图形绘制有更加精确的控制，这就需要较深入地掌握图形学理论知识，了解并熟悉专业的图形库和绘图的工具。简而言之，就是Web开发的前端主要还是关注内容和样式，图形的渲染和绘制是由浏览器底层来完成的，而可视化前端，则可能要深入底层渲染层，真正去控制图形的绘制和细节的呈现。

## 2 系统设计

最终，将各个模块融合成一个大型的系统，类似于微服务架构，对于整个系统来说，每一个模块都是独立的单元，然后每个单元都放到最后的一个容器中。系统设计是根据系统分析的结果，运用系统科学的思想和方法，设计出能最大限度满足所要求的目标 (或目的)  的新系统的过程。系统设计内容，包括确定系统功能、设计方针和方法，产生理想系统并作出草案，通过收集信息对草案作出修正产生可选设计方案，将系统分解为若干子系统，进行子系统和总系统的详细设计并进行评价，对系统方案进行论证并作出性能效果预测。

在之前已经将各个功能分离的模块设计完成了，这部分工作则是将各个模块融合为一个系统，系统设计总的原则是保证系统设计目标的实现，并在此基础上使技术资源的运用达到最佳。系统设计中，应遵循以下原则，系统性原则，经济性原则，可靠性原则，管理可接受的原则。高内聚低耦合，是软件工程中的概念，是判断软件设计好坏的标准，主要用于程序的面向对象的设计，主要看类的内聚性是否高，耦合度是否低。目的是使程序模块的可重用性、移植性大大增强。通常程序结构中各模块的内聚程度越高，模块间的耦合程度就越低。内聚是从功能角度来度量模块内的联系，一个好的内聚模块应当恰好做一件事，它描述的是模块内的功能联系；耦合是软件结构中各模块之间相互连接的一种度量，耦合强弱取决于模块间接口的复杂程度、进入或访问一个模块的点以及通过接口的数据。

系统设计的原则包括系统性原则，经济性原则，可靠性原则，管理可接受的原则。系统性原则是在系统设计中要从整个系统的角度进行考虑，注意保证系统的一致性和完整性；灵活性及可变性原则，灵活性是指系统对外界环境变化的适应能力；可靠性原则，可靠性指系统抵御外界干扰的能力及受外界干扰时的恢复能力；经济性原则，经济性是指在满足系统要求的前提下，不仅追求给用户带来一定的效益，还应尽可能减少系统不必要的开销。

## 3 测试环境检验

本阶段是在设计各个模块的同时，也在做的测试。在每个阶段都有每个阶段的测试内容。在数据爬取阶段，需要测试数据是否成功爬取，是否成功存储到了csv文件中，是否对于报出的异常进行处理；在数据处理阶段是否对爬取到的数据成功存储到了有结构的数据文件中，是否存储到Neo4j图数据库中；信息检索阶段需要测试的是，能否对用户输入的问题进行识别，能否对识别到的用户数据进行正确的处理；Web可视化阶段，需要测试的是能否在web页面输出相关的知识图谱，能否正确匹配用户想要的数据，以及用户体验如何。软件测试是使用人工或自动的手段来运行或测定某个软件系统的过程，其目的在于检验它是否满足规定的需求或弄清预期结果与实际结果之间的差别，测试阶段用到的测试环境有pycharm，unittest和selenium。

测试环境（Testing environment）是指测试运行其上的软件和硬件环境的描述，以及任何其它与被测软件交互的软件，包括驱动和桩。测试环境是指为了完成软件测试工作所必需的计算机硬件、软件、网络设备、历史数据的总称。稳定和可控的测试环境，可以使测试人员花费较少的时间就完成测试用例的执行，也无需为测试用例、测试过程的维护花费额外的时间，并且可以保证每一个被提交的缺陷都可以在任何时候被准确的重现。测试环境=软件+硬件+网络+数据准备+测试工具。简单的说，经过良好规划和管理的测试环境，可以尽可能的减少环境的变动对测试工作的不利影响，并可以对测试工作的效率和质量的提高产生积极的作用。

## 4 结论

本系统设计对于提高老师教学水平和学生论文查阅效率，对于学校师生在查阅相关资料的具有一定的实际意义。本系统的情况和价值可用于对中国知网数据和万方数据库的文献的爬取相关信息，方便了用户在查阅文献资料时不确定的查找方向，让学生和老师可以有目的性的查阅资料。本系统的优点和特色是结合了爬虫，数据分析处理，知识图谱，自然语言处理，数据可视化等各个科技领域的知识，创新点在于将各种文献资料的关系图可视化的呈现在了用户的浏览器中，数据存储性能在数据量相对较少的情况下性能处理还是比较好的，其中存在的问题和今后改进的方向主要就是在爬取的数据上和浏览器的响应速度上可以得到一定的改进，以及对自然语言处理的改进。

## 5 谢辞

首先非常感谢我的父母给予我精神上的支持，让我能够不会放弃，勇往直前，遇到不会的知识让我勇于探索，他们不断鼓励我好勤奋好学。然后非常感谢王硕老师、王兵老师在我大学的最后学习阶段——毕业设计阶段给自己的指导，从最初的定题，到资料收集，到写作、修改，到论文定稿，她们给了我耐心的指导和无私的帮助。

同时，感谢所有任课老师和所有同学在这四年来给自己的指导和帮助，是他们教会了我专业知识，教会了我如何学习，教会了我如何做人。正是由于他们，我才能在各方面取得显著的进步，在此向他们表示我由衷的谢意，并祝所有的老师培养出越来越多的优秀人才，桃李满天下！

## 6 参考文献

3. 杨瑜. 基于专长视角的学术信息检索认知能力研究[福建师范大学博士学位论文]，2016
4. 李彦. 基于Python的网络爬虫技术的研究. 电子世界，2021：39~40
5. 王启明. Python 3.7网络爬虫快速入门. 北京：清华大学出版社，2019：165~365
6. 唐松，陈智铨. Python网络爬虫从入门到实践. 北京：机械工业出版社，2017：139~267
5. 王丁. 关于自然语言处理技术的分析与研究. 科技创新导报. 2020,(07) 第141-142页
6. 赵京胜，宋梦雪，高祥. 自然语言处理发展及应用综述. 信息技术与信息化. 2019,(07)
7. 王昊奋，漆桂林. 知识图谱：方法、实践与应用.  上海：电子工业出版社，2019：197~536
8. 张帜. Neo4j权威指南（图数据库技术丛书）. 北京：清华大学出版社，2017：187~686
9. 涂铭，刘详，刘树春. Python自然语言处理实战核心技术与算法. 北京：机械工业出版社，2019：38~139
10. 王英杰. 基于知识图谱的地理实体关系构建研究.[北京建筑大学硕士专业学术论文]，2020
11. 孙洪伟，司唯山，纪兆辉. 基于本体的家谱知识图谱构建及信息检索系统的设计实现[江苏海洋大学]. 2020：156
12. 汪爱珠，马燕，项铸. 基于知识图谱的国内教育大数据可视化分析[重庆师范大学]，2020年5月：28~32
13. Xin Hu,Jiangli Duan·Depeng Dang. Natural language question answering over knowledge graph: the marriage of SPARQL query and keyword search
14. Jihong Li，Zhiqiang Wang，Yuan Wang，Zhaoyun Hua，Wenfeng Jing. Research on Distributed Search Technology of Multiple Data Sources Intelligent Information Based on Knowledge Graph
15. Guohui Xiao, Julien CormanOntology-Mediated SPARQL Query Answering over Knowledge Graphs

